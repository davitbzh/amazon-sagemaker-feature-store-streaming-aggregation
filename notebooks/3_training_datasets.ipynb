{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HSFS training datasets\n",
    "\n",
    "Training datasets is the third building block of the Hopsworks Feature Store. Data scientists can query the feature store (see [feature_exploration](./feature_exploration.ipynb) notebook) and materialize their query in training datasets.\n",
    "\n",
    "Training datasets can be saved in a ML framework friendly format (eg. TfRecords, CSV, Numpy) and then be fed to a machine learning model for training.\n",
    "\n",
    "Training datasets can also be stored on external storage systems like Amazon S3 or GCS to be read by external model training platforms.\n",
    "\n",
    "As with the previous notebooks, the first step is to establish a connection with the Hopsworks feature store and get the feature store handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from contextlib import ContextDecorator\n",
    "\n",
    "class HSFSTransformer(ContextDecorator):    \n",
    "    \n",
    "    def __init__(self, query, fg_name, feature_name, type=\"python\"):\n",
    "        self._fg_query = query\n",
    "        self._fg_name = fg_name\n",
    "        self._feature_name = feature_name\n",
    "        self._type = type\n",
    "\n",
    "    def __call__(self, f):\n",
    "        print(\"call\")\n",
    "        def wrapped_f():\n",
    "            print(\"about to call\")\n",
    "            f()\n",
    "            print(\"done calling\")\n",
    "        return wrapped_f\n",
    "\n",
    "    \"\"\"\n",
    "    def __call__(self, f):\n",
    "        @functools.wraps(f)\n",
    "        def decorated(*args, **kwds):\n",
    "            with self:\n",
    "                return f(*args, **kwds)\n",
    "        return decorated\n",
    "    \"\"\"\n",
    "    \n",
    "    def __enter__(self):\n",
    "        # sanity check\n",
    "        correct_fg = False\n",
    "        correct_ft = False\n",
    "        for feature in query._left_feature_group.features:\n",
    "            if feature_name == feature.name:\n",
    "                correct_ft = True\n",
    "                break\n",
    "        if fg_name != query._left_feature_group.name:\n",
    "            for join in query._joins:\n",
    "                for feature in join.query._left_feature_group.features:\n",
    "                    if feature_name == feature.name:\n",
    "                        correct_ft = True\n",
    "                        break\n",
    "                if fg_name == join.query._left_feature_group.name:\n",
    "                    correct_fg = True\n",
    "                    break        \n",
    "        else:\n",
    "            correct_fg = True\n",
    "        \n",
    "        if correct_fg and correct_ft:\n",
    "            # TODO (davit): check if its more efficient to read from entire query + joins or from infividual queries\n",
    "            col_object = query.read().feature_name\n",
    "                \n",
    "    def __exit__(self,exc_type,exc_value,exc_traceback):\n",
    "        print(f\"Exit Method: File Mode {self._feature_name}\")\n",
    "        #self._file.close()\n",
    "\n",
    "@HSFSTransformer\n",
    "def identity(query=\"1\", fg_name=\"a\", feature_name=\"b\"):\n",
    "    return x\n",
    "\n",
    "#inspect.getsource(identity)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "1\n",
      "b\n",
      "python\n",
      "Not access to the value returned by the __enter__ method\n",
      "Exit Method: File Mode b"
     ]
    }
   ],
   "source": [
    "identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity\n",
      "ArgSpec(args=['query', 'fg_name', 'feature_name'], varargs=None, keywords=None, defaults=('1', 'a', 'b'))\n",
      "FFFA\n",
      "Exit Method: File Mode python\n",
      "'a'"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from contextlib import ContextDecorator\n",
    "import functools \n",
    "class HSFSTransformer(ContextDecorator):    \n",
    "    \n",
    "    def __init__(self,  type=\"python\"):\n",
    "        self._type = type\n",
    "\n",
    "    def __call__(self, f):\n",
    "        @functools.wraps(f)\n",
    "        def decorated(*args, **kwds):\n",
    "            print(f.__name__)\n",
    "            print(inspect.getargspec(f))\n",
    "            with self:\n",
    "                return f(*args, **kwds)\n",
    "        return decorated\n",
    "    \n",
    "    def __enter__(self):\n",
    "        print (\"FFFA\")\n",
    "                \n",
    "    def __exit__(self,exc_type,exc_value,exc_traceback):\n",
    "        print(f\"Exit Method: File Mode {self._type}\")\n",
    "        #self._file.close()\n",
    "\n",
    "@HSFSTransformer()\n",
    "def identity(query=\"1\", fg_name=\"a\", feature_name=\"b\"):\n",
    "    return fg_name\n",
    "\n",
    "identity()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>19</td><td>application_1616513762404_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://daviteunorth-master.internal.cloudapp.net:8088/proxy/application_1616513762404_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://daviteunorth-worker-2.internal.cloudapp.net:8042/node/containerlogs/container_e14_1616513762404_0008_01_000001/realtime__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training dataset from a query\n",
    "\n",
    "In the previous notebook ([feature_exploration](./feature_exploration.ipynb)) we walked through how to explore and query the Hopsworks feature store using HSFS. We can use the queries produced in the previous notebook to create a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for getting feature group `sliced1_fg`, defaulting to `1`."
     ]
    }
   ],
   "source": [
    "sliced1_fg = fs.get_feature_group('sliced1_fg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for getting feature group `sliced_df2`, defaulting to `1`."
     ]
    }
   ],
   "source": [
    "sliced2_fg = fs.get_feature_group('sliced_df2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = sliced1_fg.select(['cc_num', 'num_trans_last_1w', 'avg_amt_last_1w']).join(sliced2_fg.select_all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------------+------------------+----------+\n",
      "|                 tid|          cc_num|num_trans_last_1w|   avg_amt_last_1w|      date|\n",
      "+--------------------+----------------+-----------------+------------------+----------+\n",
      "|4fe0a43ebe40ea4cc...|4384014188635576|               28| 600.9217857142858|2020-06-01|\n",
      "|e11a5f8ad2fc2923a...|4925013053127624|               29| 886.9306896551726|2020-05-31|\n",
      "|de0d824f943b9fc5d...|4253589902657103|               28|           1238.08|2020-05-31|\n",
      "|666dbd6c6577c0160...|4223253728365626|               31| 653.2532258064517|2020-05-31|\n",
      "|668cb927077a808fa...|4307553668184625|               30|1073.6286666666667|2020-05-31|\n",
      "+--------------------+----------------+-----------------+------------------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "sliced1_fg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Use udf to define a row-at-a-time udf\n",
    "@udf('double')\n",
    "# Input/output are both a single double value\n",
    "def plus_one(v):\n",
    "      return v + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------------+------------------+----------+------------------+\n",
      "|                 tid|          cc_num|num_trans_last_1w|   avg_amt_last_1w|      date|                v2|\n",
      "+--------------------+----------------+-----------------+------------------+----------+------------------+\n",
      "|4fe0a43ebe40ea4cc...|4384014188635576|               28| 600.9217857142858|2020-06-01| 601.9217857142858|\n",
      "|e11a5f8ad2fc2923a...|4925013053127624|               29| 886.9306896551726|2020-05-31| 887.9306896551726|\n",
      "|de0d824f943b9fc5d...|4253589902657103|               28|           1238.08|2020-05-31|           1239.08|\n",
      "|666dbd6c6577c0160...|4223253728365626|               31| 653.2532258064517|2020-05-31| 654.2532258064517|\n",
      "|668cb927077a808fa...|4307553668184625|               30|1073.6286666666667|2020-05-31|1074.6286666666667|\n",
      "|5f4fd652aa930cb08...|4281783875596457|               20| 585.2560000000001|2020-05-31| 586.2560000000001|\n",
      "|045c7b3d51ae6df50...|4794233751173765|               26| 566.1526923076922|2020-05-31| 567.1526923076922|\n",
      "|ae3fc0640b8f7c2b8...|4858129660270572|               25|          280.4052|2020-05-31|          281.4052|\n",
      "|7a980ecd61759ad58...|4978667132535671|               29| 657.0896551724137|2020-05-31| 658.0896551724137|\n",
      "|7923703c523848375...|4170245277417751|               33| 649.0357575757575|2020-05-31| 650.0357575757575|\n",
      "|58076b02a1fdbcf19...|4148950609641791|               24|          378.6775|2020-05-31|          379.6775|\n",
      "|994b104c121929292...|4671096685272336|               20|           528.637|2020-05-31|           529.637|\n",
      "|0abc39fc2cffb5d6a...|4956860373932956|               29|  770.059655172414|2020-05-31|  771.059655172414|\n",
      "|e66254f2999bacee1...|4827807245873162|               35| 719.7405714285715|2020-05-31| 720.7405714285715|\n",
      "|da6c53bfac9129d43...|4161715127983823|               22| 532.4986363636364|2020-05-31| 533.4986363636364|\n",
      "|1e01100a8b51c2525...|4208317936968510|               20| 591.7260000000001|2020-05-31| 592.7260000000001|\n",
      "|b78b252cb0fed19cc...|4259753809723533|               29|1315.8644827586204|2020-05-31|1316.8644827586204|\n",
      "|75124fe6657bdfd60...|4671956999823027|               34|189.07647058823534|2020-05-31|190.07647058823534|\n",
      "|813461ac4fbce061e...|4526611032294580|               23|1081.2743478260873|2020-05-31|1082.2743478260873|\n",
      "|28e12670c72eed4d3...|4789490563144262|               30| 541.5566666666666|2020-05-31| 542.5566666666666|\n",
      "+--------------------+----------------+-----------------+------------------+----------+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df = sliced1_fg.read()\n",
    "df = df.withColumn('v2', plus_one(df.avg_amt_last_1w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o405.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 93, daviteunorth-worker-5.internal.cloudapp.net, executor 5): java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 378, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o405.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 4 times, most recent failure: Lost task 0.3 in stage 14.0 (TID 93, daviteunorth-worker-5.internal.cloudapp.net, executor 5): java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.IllegalArgumentException\n",
      "\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:334)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageSerializer.readMessage(MessageSerializer.java:543)\n",
      "\tat org.apache.arrow.vector.ipc.message.MessageChannelReader.readNext(MessageChannelReader.java:58)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowStreamReader.readSchema(ArrowStreamReader.java:132)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.initialize(ArrowReader.java:181)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.ensureInitialized(ArrowReader.java:172)\n",
      "\tat org.apache.arrow.vector.ipc.ArrowReader.getVectorSchemaRoot(ArrowReader.java:65)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "def pandas_plus_one(v):\n",
    "    return v + 1\n",
    "\n",
    "df = df.withColumn('v3', pandas_plus_one(df.avg_amt_last_1w))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<hsfs.constructor.join.Join object at 0x7f5cf05e20d0>]"
     ]
    }
   ],
   "source": [
    "query._joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sliced_df2\n",
      "fraud_label\n",
      "amt_ratio2\n",
      "datetime\n",
      "amount\n",
      "count_ratio\n",
      "tid\n",
      "amt_ratio1\n",
      "num_trans_last_10m\n",
      "avg_amt_last_10m\n",
      "date"
     ]
    }
   ],
   "source": [
    "for join in query._joins:\n",
    "    print(join.query._left_feature_group.name)\n",
    "    for i in join.query._left_feature_group.features:\n",
    "        print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7f5cf05bce50>"
     ]
    }
   ],
   "source": [
    "query._left_feature_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sliced1_fg'"
     ]
    }
   ],
   "source": [
    "query._left_feature_group.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    }
   ],
   "source": [
    "query._left_feature_group.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc_num\n",
      "tid\n",
      "num_trans_last_1w\n",
      "avg_amt_last_1w\n",
      "date"
     ]
    }
   ],
   "source": [
    "for i in query._left_feature_group.features:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "query._left_feature_group.online_enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feature('cc_num', None, None, False, False, None, None, None), Feature('num_trans_last_1w', None, None, False, False, None, None, None), Feature('avg_amt_last_1w', None, None, False, False, None, None, None)]"
     ]
    }
   ],
   "source": [
    "query._left_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "sliced_query._joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'cc_num'>"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)\n",
    "def subtract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=v - v.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=LongType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the feature groups, we first need to generate a metadata object representing the training dataset. After that, we can call the `save()` method to persist it in the Hopsworks feature store.\n",
    "Different file formats are available: `csv`, `tfrecord`, `npy`, `hdf5`, `avro`, `parquet`, `orc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f94701b38d0>"
     ]
    }
   ],
   "source": [
    "td = fs.create_training_dataset(name=\"sales_model\",\n",
    "                               description=\"Dataset to train the sales model\",\n",
    "                               data_format=\"csv\",\n",
    "                               version=1)\n",
    "\n",
    "td.save(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass write options\n",
    "\n",
    "When you save a training dataset, you have the possibility of specifying additional parameters to the Spark writer. For instance, in the example below, we are adding the headers to the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f948851b610>"
     ]
    }
   ],
   "source": [
    "td = fs.create_training_dataset(name=\"sales_model\",\n",
    "                               description=\"Dataset to train the sales model\",\n",
    "                               data_format=\"csv\",\n",
    "                               version=2)\n",
    "\n",
    "td.save(query, {'hearder': 'true'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the training dataset\n",
    "\n",
    "If you are training a model, you might want to split the training datasets into different slices (training, test and validation). HSFS allows you to specify the split sizes. You can also provide a seed for the random splitter, if you want to reproduce a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f9488528510>"
     ]
    }
   ],
   "source": [
    "td = fs.create_training_dataset(name=\"sales_model\",\n",
    "                               description=\"Dataset to train the sales model\",\n",
    "                               data_format=\"csv\",\n",
    "                               splits={'train': 0.7, 'test': 0.2, 'validate': 0.1},\n",
    "                               version=3)\n",
    "\n",
    "td.save(query, {'hearder': 'true'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dataset on an external storage system\n",
    "\n",
    "If you are training your model on an external machine learning platform (e.g. SageMaker), you might want to save the training dataset on an external storage system (e.g. S3). You can take advantage of the Hopsworks storage connectors (see [documentation](https://hopsworks.readthedocs.io/en/latest/featurestore/guides/featurestore.html#configuring-storage-connectors-for-the-feature-store)).\n",
    "\n",
    "Assuming you have created an S3 storage connector name `td_bucket_connector`, you can create an external training dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_bucket_connector = fs.get_storage_connector(\"td_bucket_connector\", \"S3\")\n",
    "\n",
    "td = fs.create_training_dataset(name=\"sales_model\",\n",
    "                               description=\"Dataset to train the sales model\",\n",
    "                               data_format=\"csv\",\n",
    "                               storage_connector=td_bucket_connector,\n",
    "                               version=4)\n",
    "\n",
    "### This code is expected to fail if you connector is not configured properly\n",
    "td.save(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay the query that generated the training dataset\n",
    "\n",
    "If you created a training dataset from a query object, then you can ask the feature store to return the set of features (in order) and the set of joins that generated. \n",
    "This feature is useful if you are serving a model in production and you want to augment the inference vector with features taken from the online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT `fg0`.`sales_last_quarter_store_dep`, `fg0`.`store`, `fg0`.`sales_last_month_store_dep`, `fg0`.`sales_last_year_store`, `fg0`.`sales_last_quarter_store`, `fg0`.`sales_last_six_month_store_dep`, `fg0`.`sales_last_year_store_dep`, `fg0`.`sales_last_month_store`, `fg0`.`dept`, `fg0`.`sales_last_six_month_store`, `fg0`.`weekly_sales`, `fg0`.`is_holiday`, `fg0`.`date`, `fg1`.`fuel_price`, `fg1`.`unemployment`, `fg1`.`cpi`\n",
      "FROM `demo_fs_meb10000`.`sales_fg_1` `fg0`\n",
      "INNER JOIN `demo_fs_meb10000`.`exogenous_fg_1` `fg1` ON `fg0`.`date` = `fg1`.`date` AND `fg0`.`store` = `fg1`.`store`\n",
      "VersionWarning: No version provided for getting training dataset `sales_model`, defaulting to `1`."
     ]
    }
   ],
   "source": [
    "td = fs.get_training_dataset(name=\"sales_model\")\n",
    "print(td.query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training dataset from a DataFrame\n",
    "\n",
    "If you need to apply additional transformations before creating a training dataset, you can create one from a Spark DataFrame instead of using a query.\n",
    "The `create_training_dataset` part stays the same, the difference is that we are going to pass a DataFrame to the `save()` method.\n",
    "\n",
    "As you have applied additional transformations between the query object and the training dataset, we won't be able to re-play the query for this specific training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df = query.read()\n",
    "# Apply additional transformations\n",
    "df = (df.withColumn(\"is_holiday\", F.when(F.col(\"is_holiday\") == \"true\", 1 ).otherwise(0))\n",
    "       .withColumn(\"unemployment\", F.col(\"unemployment\").cast(\"double\")) \n",
    "       .withColumn(\"cpi\", F.col(\"cpi\").cast(\"double\"))\n",
    "       .drop(\"date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save as `csv` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f94884e0150>"
     ]
    }
   ],
   "source": [
    "td = fs.create_training_dataset(name=\"sales_model\",\n",
    "                               description=\"Dataset to train the sales model\",\n",
    "                               data_format=\"csv\",\n",
    "                               splits={'train': 0.7, 'test': 0.2, 'validate': 0.1},                                \n",
    "                               version=5)\n",
    "\n",
    "td.save(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save as `tfrecord` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f948849f210>"
     ]
    }
   ],
   "source": [
    "td = fs.create_training_dataset(name=\"sales_model\",\n",
    "                               description=\"Dataset to train the sales model\",\n",
    "                               data_format=\"tfrecord\",\n",
    "                               splits={'train': 0.7, 'test': 0.2, 'validate': 0.1},                                \n",
    "                               version=6)\n",
    "\n",
    "td.save(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a tag to a training dataset\n",
    "\n",
    "As for feature groups, you can add tags to a training dataset. Tags are indexed and you can search for them in the Hopsworks feature store UI. Tags are an useful tool to catalog the feature store. The `value` field can be omitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Training Datasets\n",
    "The feature store enables users to attach tags to training dataset in order to make them discoverable across feature\n",
    "stores.  A tag is a simple {key: value} association, providing additional information about the data, such as for\n",
    "example geographic origin. This is useful in an organization as it makes easier to discover for data scientists, reduces\n",
    "duplicated work in terms of for example data preparation. The tagging feature is only available in the enterprise version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define tags that can be attached\n",
    "The first step is to define a set of tags that can be attached. Such as for example Country to tag data as being from\n",
    "a certain geographic location and Sport to further associate a type of Sport with the data.\n",
    "\n",
    "![Define tags that can be attached](images/creating_tags.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attach tags using the UI\n",
    "Tags can then be attached using the feature store UI or programmatically using the API.\n",
    "Attaching tags to feature group.\n",
    "\n",
    "![Attach tags using the UI](images/attach_tags.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = fs.get_training_dataset(\"sales_model\", 5)\n",
    "td.add_tag(\"model\", value=\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the HSFS API you can also list all the tags associated with a specific training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'model': 'sales'}]"
     ]
    }
   ],
   "source": [
    "td = fs.get_training_dataset(\"sales_model\", 5)\n",
    "td.get_tag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a training dataset\n",
    "\n",
    "As for feature groups, you can call the methods `show()` method to get a preview of the training dataset and `read()` to get a Spark DataFrame of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----+--------------------------+---------------------+------------------------+------------------------------+-------------------------+----------------------+----+--------------------------+------------+----------+-------------------+----------+------------+-----------+\n",
      "|sales_last_quarter_store_dep|store|sales_last_month_store_dep|sales_last_year_store|sales_last_quarter_store|sales_last_six_month_store_dep|sales_last_year_store_dep|sales_last_month_store|dept|sales_last_six_month_store|weekly_sales|is_holiday|               date|fuel_price|unemployment|        cpi|\n",
      "+----------------------------+-----+--------------------------+---------------------+------------------------+------------------------------+-------------------------+----------------------+----+--------------------------+------------+----------+-------------------+----------+------------+-----------+\n",
      "|                         0.0|   20|                       0.0|                  0.0|                     0.0|                           0.0|                      0.0|                   0.0|  55|                       0.0|    32362.95|     false|2010-02-05 00:00:00|     2.784|       8.187|204.2471935|\n",
      "|                         0.0|   20|                       0.0|                  0.0|                     0.0|                           0.0|                      0.0|                   0.0|  94|                       0.0|    63787.83|     false|2010-02-05 00:00:00|     2.784|       8.187|204.2471935|\n",
      "|                         0.0|   20|                       0.0|                  0.0|                     0.0|                           0.0|                      0.0|                   0.0|  22|                       0.0|    17597.83|     false|2010-02-05 00:00:00|     2.784|       8.187|204.2471935|\n",
      "|                         0.0|   20|                       0.0|                  0.0|                     0.0|                           0.0|                      0.0|                   0.0|  30|                       0.0|     9488.37|     false|2010-02-05 00:00:00|     2.784|       8.187|204.2471935|\n",
      "|                         0.0|   20|                       0.0|                  0.0|                     0.0|                           0.0|                      0.0|                   0.0|   2|                       0.0|    85812.69|     false|2010-02-05 00:00:00|     2.784|       8.187|204.2471935|\n",
      "+----------------------------+-----+--------------------------+---------------------+------------------------+------------------------------+-------------------------+----------------------+----+--------------------------+------------+----------+-------------------+----------+------------+-----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "td = fs.get_training_dataset(\"sales_model\", 2)\n",
    "td.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have splitted your training dataset, you can also read a single split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295125"
     ]
    }
   ],
   "source": [
    "td = fs.get_training_dataset(\"sales_model\", 6)\n",
    "td.read(\"train\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input the training dataset to a model training loop\n",
    "If you are training a model, HSFS provides `tf_data` method that returns `TFDataEngine` object with utility methods to read training dataset as `tf.data.Dataset` object to read the training dataset and feed it to a model training loop efficiently. \n",
    "* Currently `TFDataEngine` provides 2 utility methods `tf_record_dataset` and `tf_csv_dataset` for reading `.tfrecord` and `.csv` files, respectivelly.\n",
    "* Both methods support only following feature types `string`, `short`, `int`, `long`, `float` and `double`.\n",
    "* In both methods you can set `process` argument to `True` and they will return `PrefetchDataset` ready to input to model training loop.\n",
    "* If you would like to apply your own logic to feature transformation using `tf.data.Dataset` then set `process` argument to `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### proccess using `tf_record_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = td.tf_data(target_name='weekly_sales', split='train', is_training=True)\n",
    "train_input_processed = train_input.tf_record_dataset(process=True, batch_size =32, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((32, 14), (32,)), types: (tf.float32, tf.float32)>"
     ]
    }
   ],
   "source": [
    "train_input_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply custom logic to `tf_record_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = fs.get_training_dataset(\"sales_model\", 6)\n",
    "\n",
    "train_input = td.tf_data(target_name=None, split='train', is_training=True)\n",
    "train_input_not_processed = train_input.tf_record_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ParallelMapDataset shapes: {cpi: (), dept: (), fuel_price: (), is_holiday: (), sales_last_month_store: (), sales_last_month_store_dep: (), sales_last_quarter_store: (), sales_last_quarter_store_dep: (), sales_last_six_month_store: (), sales_last_six_month_store_dep: (), sales_last_year_store: (), sales_last_year_store_dep: (), store: (), unemployment: (), weekly_sales: ()}, types: {cpi: tf.float32, dept: tf.int64, fuel_price: tf.float32, is_holiday: tf.int64, sales_last_month_store: tf.float32, sales_last_month_store_dep: tf.float32, sales_last_quarter_store: tf.float32, sales_last_quarter_store_dep: tf.float32, sales_last_six_month_store: tf.float32, sales_last_six_month_store_dep: tf.float32, sales_last_year_store: tf.float32, sales_last_year_store_dep: tf.float32, store: tf.int64, unemployment: tf.float32, weekly_sales: tf.float32}>"
     ]
    }
   ],
   "source": [
    "train_input_not_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 1 \n",
    "\n",
    "def custom_impl(example):\n",
    "    feature_names = [td_feature.name for td_feature in td.schema] \n",
    "    label_name = feature_names.pop(feature_names.index('weekly_sales'))\n",
    "    x = [tf.cast(example[feature_name], tf.float32) for feature_name in feature_names]\n",
    "    y = example[label_name]\n",
    "    return x,y\n",
    "\n",
    "train_input_custum_processed = train_input_not_processed.map(lambda value: custom_impl(value))\\\n",
    "    .shuffle(num_epochs * batch_size)\\\n",
    "    .repeat(num_epochs * batch_size)\\\n",
    "    .cache()\\\n",
    "    .batch(batch_size, drop_remainder=True)\\\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((32, 14), (32,)), types: (tf.float32, tf.float32)>"
     ]
    }
   ],
   "source": [
    "train_input_custum_processed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
