{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>11</td><td>application_1616109604682_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/http://hopsworks0.logicalclocks.com:8088/proxy/application_1616109604682_0003/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/http://hopsworks0.logicalclocks.com:8042/node/containerlogs/container_e05_1616109604682_0003_01_000001/test__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fe922293fd0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "import datetime\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "\n",
    "connection = hsfs.connection()\n",
    "# get a reference to the feature store, you can access also shared feature stores by providing the feature store name\n",
    "fs = connection.get_feature_store();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_fg_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"salary\", FloatType(), True),\n",
    "  StructField(\"commission\", FloatType(), True),\n",
    "  StructField(\"car\", StringType(), True), \n",
    "  StructField(\"hvalue\", FloatType(), True),      \n",
    "  StructField(\"hyears\", IntegerType(), True),     \n",
    "  StructField(\"loan\", FloatType(), True),\n",
    "  StructField(\"year\", IntegerType(), True)    \n",
    "])\n",
    "\n",
    "demographic_fg_schema = StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"age\", IntegerType(), True),\n",
    "  StructField(\"elevel\", StringType(), True),   \n",
    "  StructField(\"zipcode\", StringType(), True)     \n",
    "])\n",
    "\n",
    "class_fg_schema =  StructType([\n",
    "  StructField(\"id\", IntegerType(), True),\n",
    "  StructField(\"class\", StringType(), True),\n",
    "  StructField(\"year\", IntegerType(), True)              \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create spark dataframes for each Feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_bulk_insert_data = [\n",
    "    Row(1, 110499.73, 0.0,  \"car15\",  235000.0, 30, 354724.18, 2020),\n",
    "    Row(2, 140893.77, 0.0,  \"car20\",  135000.0, 2, 395015.33, 2020),\n",
    "    Row(3, 119159.65, 0.0,  \"car1\", 145000.0, 22, 122025.08, 2020),\n",
    "    Row(4, 20000.0, 52593.63, \"car9\", 185000.0, 30, 99629.62, 2020)    \n",
    "]\n",
    "\n",
    "economy_bulk_insert_df = spark.createDataFrame(economy_bulk_insert_data, economy_fg_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "| id|   salary|commission|  car|  hvalue|hyears|     loan|year|\n",
      "+---+---------+----------+-----+--------+------+---------+----+\n",
      "|  1|110499.73|       0.0|car15|235000.0|    30| 354724.2|2020|\n",
      "|  2|140893.77|       0.0|car20|135000.0|     2|395015.34|2020|\n",
      "|  3|119159.65|       0.0| car1|145000.0|    22|122025.08|2020|\n",
      "|  4|  20000.0|  52593.63| car9|185000.0|    30| 99629.62|2020|\n",
      "+---+---------+----------+-----+--------+------+---------+----+"
     ]
    }
   ],
   "source": [
    "economy_bulk_insert_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_bulk_insert_data = [\n",
    "    Row(1, 54, \"level3\", \"zipcode5\"),\n",
    "    Row(2, 44, \"level4\", \"zipcode8\"),\n",
    "    Row(3, 49, \"level2\", \"zipcode4\"),\n",
    "    Row(4, 56, \"level0\", \"zipcode2\")    \n",
    "]\n",
    "\n",
    "demographic_bulk_insert_df = spark.createDataFrame(demographic_bulk_insert_data, demographic_fg_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+--------+\n",
      "| id|age|elevel| zipcode|\n",
      "+---+---+------+--------+\n",
      "|  1| 54|level3|zipcode5|\n",
      "|  2| 44|level4|zipcode8|\n",
      "|  3| 49|level2|zipcode4|\n",
      "|  4| 56|level0|zipcode2|\n",
      "+---+---+------+--------+"
     ]
    }
   ],
   "source": [
    "demographic_bulk_insert_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_bulk_insert_data = [\n",
    "    Row(1, \"groupB\", 2020),\n",
    "    Row(2, \"groupB\", 2020),\n",
    "    Row(3, \"groupB\", 2020),\n",
    "    Row(4, \"groupB\", 2020)    \n",
    "]\n",
    "\n",
    "class_bulk_insert_df = spark.createDataFrame(class_bulk_insert_data, class_fg_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one function per feature\n",
    "@hsfs_transformer(fg_name=\"fg1\", fg_v = 1, feature_name=\"ft1\")\n",
    "def f1(x):\n",
    "    \"your logic here\"\n",
    "    return int(x)\n",
    "\n",
    "@hsfs_transformer(fg_name=\"fg2\",fg_v = 2, feature_name=\"ft2\")\n",
    "def f2(x):\n",
    "    \"your logic here\"\n",
    "    return str(x)\n",
    "\n",
    "@hsfs_transformer(\"fg3\", fg_v = 3, feature_name=\"ft3\")\n",
    "def f3(x):\n",
    "    \"your logic here\"\n",
    "    return float(x)\n",
    "\n",
    "f4 = get_function(\"f4\",1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(fg_name1,feature_name1, fg_name2,feature_name2, fg_name3,feature_name3):\n",
    "    f1(fg_name1,feature_name1)\n",
    "    f2(fg_name2,feature_name2)\n",
    "    f3(fg_name3,feature_name3)\n",
    "\n",
    "pipe([ft1,ft2,ft3,f4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some():\n",
    "    select() \n",
    "    int(x)\n",
    "    str(x)\n",
    "    float(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0, 0.1 0. 1\n",
    "\n",
    "0 0.1\n",
    "0.01\n",
    "0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(object):\n",
    "    def __init__(self):\n",
    "        print(\"[Transformer] Initializing...\")\n",
    "    def preprocess(self, inputs):\n",
    "        print(\"[Transformer] Preprocess:\")\n",
    "        print(inputs)\n",
    "        inputs -> primary dict_keys\n",
    "        feature_vector = hsfs.get_serving_vector(dict_keys)\n",
    "        trans_functions = hsfs.get_training_dataset(os.env[MODEL_NAME]).get_trans_func()\n",
    "        schema = hsfs.get_training_dataset().td_query\n",
    "        for trans_f in trans_functions:\n",
    "            f_id = schema(trans_f.feature_name)\n",
    "            results[f_id] = trans_f(feature_vector[f_id])\n",
    "        pipe_trans_function = hsfs.get_training_dataset(os.env[MODEL_NAME]).get_trans_func()\n",
    "        results = pipe_trans_function(feature_vector)\n",
    "        return results\n",
    "    def postprocess(self, outputs):\n",
    "        print(\"[Transformer] Postprocess and return...\")\n",
    "        print(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+\n",
      "| id| class|year|\n",
      "+---+------+----+\n",
      "|  1|groupB|2020|\n",
      "|  2|groupB|2020|\n",
      "|  3|groupB|2020|\n",
      "|  4|groupB|2020|\n",
      "+---+------+----+"
     ]
    }
   ],
   "source": [
    "class_bulk_insert_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_td = fs.create_training_dataset(\n",
    "    name = \"economy_td\", \n",
    "    description = \"Hudi Household Economy Feature Group\",\n",
    "    version=1,\n",
    "    data_format='csv'\n",
    "    transformer_fn = [f1,f2,f3,f4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7fe8bc73c390>"
     ]
    }
   ],
   "source": [
    "economy_td.save(class_bulk_insert_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_td.attach_transformer_fn([f1,f2,f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_td.detach_transformer_fn([f1,f2,f2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7fe8bc73c390>"
     ]
    }
   ],
   "source": [
    "economy_td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_td.attach_transformer_fn(identity, type=\"python\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_td.get_transformer_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Expecting value: line 1 column 1 (char 0)\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/decorators.py\", line 35, in if_connected\n",
      "    return fn(inst, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/client/base.py\", line 155, in _send_request\n",
      "    return response.json()\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/requests/models.py\", line 889, in json\n",
      "    self.content.decode(encoding), **kwargs\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/json/__init__.py\", line 348, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/json/decoder.py\", line 337, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/json/decoder.py\", line 355, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from hsfs import transformer_function\n",
    "from hsfs import client\n",
    "transformer_instance = transformer_function.TransformerFunction(\n",
    "                                        name=identity.__name__,\n",
    "                                        version=1,\n",
    "                                        transformer_location='',\n",
    "                                        transformer_type='python'\n",
    "            )\n",
    "        \n",
    "_client = client.get_instance()\n",
    "path_params = [\n",
    "            \"project\",\n",
    "            _client._project_id,\n",
    "            \"featurestores\",\n",
    "            67,\n",
    "            \"trainingdatasets\",\n",
    "            economy_td.id,\n",
    "            \"transformerFunctions\",\n",
    "        ]\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "\n",
    "_client._send_request(\n",
    "                \"POST\",\n",
    "                path_params,\n",
    "                headers=headers,\n",
    "                data=transformer_instance.json()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'{\"name\": null, \"version\": null, \"transformerLocation\": null, \"transformerType\": null}'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_td = fs.get_training_dataset(\"economy_td\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "economy_td.attach_transformation_function(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19"
     ]
    }
   ],
   "source": [
    "economy_td.transformation_function(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs.core import training_dataset_engine\n",
    "_training_dataset_engine = training_dataset_engine.TrainingDatasetEngine(67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "[Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/core/training_dataset_engine.py\", line 210, in get_transformation_function\n",
      "    hdfs.load(transformer_fn_instance.transformer_location)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/pydoop/hdfs/__init__.py\", line 143, in load\n",
      "    with open(hdfs_path, **kwargs) as fi:\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/pydoop/hdfs/__init__.py\", line 113, in open\n",
      "    encoding, errors)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/pydoop/hdfs/fs.py\", line 280, in open_file\n",
      "    f = self.fs.open_file(path, m, buff_size, replication, blocksize)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_transformation_function = _training_dataset_engine.get_transformation_function(economy_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19"
     ]
    }
   ],
   "source": [
    "_transformation_function(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<hsfs.transformer_function.TransformerFunction object at 0x7fe8bbafcf50>]"
     ]
    }
   ],
   "source": [
    "from hsfs.core import training_dataset_api\n",
    "_training_dataset_api = training_dataset_api.TrainingDatasetApi(\n",
    "            67\n",
    "        )\n",
    "_training_dataset_api.get_transformation_function(economy_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ContextDecorator\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HsfsTransormation(ContextDecorator):\n",
    "    \"\"\"Timing decorator.\"\"\"\n",
    "\n",
    "    def __init__(self, description):\n",
    "        self.description = description\n",
    "\n",
    "    def __enter__(self):\n",
    "        print(self.description)\n",
    "        self.start_time = time()\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end_time = time()\n",
    "        run_time = self.end_time - self.start_time\n",
    "        print(f\"The function took {run_time} seconds to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PysparkTransormation(HsfsTransormation):\n",
    "    def __init__(self):\n",
    "        super().__init__(description=\"pyspark\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "@PysparkTransormation()\n",
    "def identity_tnx(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark\n",
      "The function took 5.245208740234375e-06 seconds to run.\n",
      "19"
     ]
    }
   ],
   "source": [
    "identity_tnx(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create meta data object here\n",
      "Whee!\n",
      "Something is happening after the function is called."
     ]
    }
   ],
   "source": [
    "def hsfs_transormation(func):\n",
    "    def wrapper():\n",
    "        print(\"create meta data object here\")\n",
    "        func()\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "@hsfs_transormation\n",
    "def say_whee():\n",
    "    print(\"Whee!\")\n",
    "\n",
    "say_whee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    " \n",
    "class AbstractClassExample(ABC):\n",
    " \n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        super().__init__()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def do_something(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "420"
     ]
    }
   ],
   "source": [
    "class DoAdd42(AbstractClassExample):\n",
    "\n",
    "    def do_something(self):\n",
    "        return self.value + 42\n",
    "    \n",
    "class DoMul42(AbstractClassExample):\n",
    "   \n",
    "    def do_something(self):\n",
    "        return self.value * 42\n",
    "    \n",
    "x = DoAdd42(10)\n",
    "y = DoMul42(10)\n",
    "\n",
    "print(x.do_something())\n",
    "print(y.do_something())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    " \n",
    "class AbstractClassExample(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def do_something(self):\n",
    "        print(\"Some implementation!\")\n",
    "        \n",
    "class AnotherSubclass(AbstractClassExample):\n",
    "\n",
    "    def do_something(self):\n",
    "        super().do_something()\n",
    "        print(\"The enrichment from AnotherSubclass\")\n",
    "        \n",
    "x = AnotherSubclass()\n",
    "x.do_something()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
