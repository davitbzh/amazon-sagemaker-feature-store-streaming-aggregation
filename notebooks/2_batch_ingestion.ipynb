{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>17</td><td>application_1616513762404_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://daviteunorth-master.internal.cloudapp.net:8088/proxy/application_1616513762404_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://daviteunorth-worker-2.internal.cloudapp.net:8042/node/containerlogs/container_e14_1616513762404_0006_01_000001/realtime__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to Hopsworks Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import desc, dense_rank\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from  argparse import Namespace, ArgumentParser\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "TOTAL_UNIQUE_USERS = 100 #10000\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "    \n",
    "schema = StructType([StructField('tid', StringType(), True),\n",
    "                         StructField('datetime', TimestampType(), True),\n",
    "                         StructField('cc_num', LongType(), True),\n",
    "                         StructField('amount', DoubleType(), True),\n",
    "                         StructField('fraud_label', StringType(), True)])\n",
    "\n",
    "# aggregated_features\n",
    "#transactions_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"hdfs:///Projects/realtime/Resources/transactions.csv\")\n",
    "\n",
    "transactions_df = spark.read.csv(path=\"hdfs:///Projects/realtime/Resources/transactions.csv\", inferSchema=True, header=True, sep=\",\") #schema=schema, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- tid: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-------------------+-----------+--------------------+\n",
      "|amount|          cc_num|           datetime|fraud_label|                 tid|\n",
      "+------+----------------+-------------------+-----------+--------------------+\n",
      "|  19.9|4653672048903767|2020-03-15 15:06:09|          0|df9058c1c293e6bd3...|\n",
      "|  36.6|4170245277417751|2020-03-15 15:10:11|          0|b9902ddccead2a1ca...|\n",
      "| 26.28|4978667132535671|2020-03-15 15:20:11|          0|2985420e04a29c7e1...|\n",
      "|822.53|4332149413304557|2020-03-15 15:20:31|          0|e41a745ed61f9d875...|\n",
      "| 652.2|4676706014866559|2020-03-15 15:26:45|          0|34108c0ff7bb8af9b...|\n",
      "| 48.34|4978667132535671|2020-03-15 15:36:04|          0|3997b0cb3d7d39f02...|\n",
      "| 18.55|4444037300542691|2020-03-15 15:41:04|          0|ac80ce12bcc81aa6e...|\n",
      "| 67.22|4789490563144262|2020-03-15 15:45:31|          0|dd3a6f7ec38653709...|\n",
      "|971.94|4829328237114208|2020-03-15 16:05:12|          0|553359d0e3e453bce...|\n",
      "| 89.96|4829328237114208|2020-03-15 16:06:34|          0|f8317ecbb278d27df...|\n",
      "| 84.54|4645884898081724|2020-03-15 16:22:58|          0|4522f2106ec6f2c33...|\n",
      "|256.21|4148950609641791|2020-03-15 16:24:13|          0|4e776b65a875d93f3...|\n",
      "| 34.44|4978667132535671|2020-03-15 16:25:02|          0|5ac88881ac009d9ac...|\n",
      "| 99.04|4301684712758167|2020-03-15 16:28:03|          0|3d842a0a13798e5f8...|\n",
      "|  4.42|4564139086560436|2020-03-15 16:32:47|          0|91967b945c4a43d9c...|\n",
      "|  48.7|4526611032294580|2020-03-15 16:32:56|          0|71385e99dceaf1aff...|\n",
      "|  42.8|4446288810992896|2020-03-15 16:44:07|          0|3a748dd997a473163...|\n",
      "|  1.64|4223253728365626|2020-03-15 16:49:15|          0|adbd1a216df5e6d05...|\n",
      "| 64.31|4532606684286514|2020-03-15 16:49:37|          0|d958d53d2ef32a904...|\n",
      "| 36.37|4522929944170142|2020-03-15 16:51:15|          0|959da5e697859c158...|\n",
      "+------+----------------+-------------------+-----------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT *, \\\n",
    "           avg_amt_last_10m/avg_amt_last_1w AS amt_ratio1, \\\n",
    "           amount/avg_amt_last_1w AS amt_ratio2, \\\n",
    "           num_trans_last_10m/num_trans_last_1w AS count_ratio \\\n",
    "    FROM \\\n",
    "        ( \\\n",
    "        SELECT *, \\\n",
    "               COUNT(*) OVER w1 as num_trans_last_10m, \\\n",
    "               AVG(amount) OVER w1 as avg_amt_last_10m, \\\n",
    "               COUNT(*) OVER w2 as num_trans_last_1w, \\\n",
    "               AVG(amount) OVER w2 as avg_amt_last_1w \\\n",
    "        FROM transactions_df \\\n",
    "        WINDOW \\\n",
    "               w1 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 10 MINUTE PRECEDING), \\\n",
    "               w2 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 1 WEEK PRECEDING) \\\n",
    "        ) \n",
    "    \"\"\"\n",
    "transactions_df.registerTempTable('transactions_df')\n",
    "aggregated_features = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_by_card_number\n",
    "window = Window.partitionBy('cc_num').orderBy(desc('datetime'))\n",
    "sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "grouped_df = sorted_df.filter(sorted_df.rank == 1).drop(sorted_df.rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- num_trans_last_10m: long (nullable = false)\n",
      " |-- avg_amt_last_10m: double (nullable = true)\n",
      " |-- num_trans_last_1w: long (nullable = false)\n",
      " |-- avg_amt_last_1w: double (nullable = true)\n",
      " |-- amt_ratio1: double (nullable = true)\n",
      " |-- amt_ratio2: double (nullable = true)\n",
      " |-- count_ratio: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "grouped_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           datetime|\n",
      "+-------------------+\n",
      "|2020-05-31 18:31:38|\n",
      "|2020-05-31 19:57:01|\n",
      "|2020-05-31 20:45:54|\n",
      "|2020-05-31 23:43:01|\n",
      "|2020-05-31 23:37:09|\n",
      "+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "grouped_df.select(\"datetime\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "grouped_df = grouped_df.withColumn(\"date\", f.from_unixtime(f.unix_timestamp(f.col(\"datetime\")), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "grouped_df.select(\"date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df1 = grouped_df.select('tid', 'date', 'cc_num', 'num_trans_last_1w', 'avg_amt_last_1w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_df2 = grouped_df.drop('cc_num', 'num_trans_last_1w', 'avg_amt_last_1w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "connection = hsfs.connection()\n",
    "# get a reference to the feature store, you can access also shared feature stores by providing the feature store name\n",
    "fs = connection.get_feature_store();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fb121d7abd0>"
     ]
    }
   ],
   "source": [
    "sliced1_fg = fs.create_feature_group(\n",
    "    name = \"sliced1_fg\", \n",
    "    description = \"Transactions Feature Group\",\n",
    "    version=1,\n",
    "    primary_key = [\"tid\"], \n",
    "    partition_key = [\"date\"], \n",
    "    time_travel_format = None\n",
    ")\n",
    "sliced1_fg.save(sliced_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fb10dc97cd0>"
     ]
    }
   ],
   "source": [
    "sliced2_fg = fs.create_feature_group(\n",
    "    name = \"sliced_df2\", \n",
    "    description = \"Transactions Feature Group\",\n",
    "    version=1,\n",
    "    primary_key = [\"tid\"], \n",
    "    partition_key = [\"date\"], \n",
    "    time_travel_format = None\n",
    ")\n",
    "sliced2_fg.save(sliced_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
