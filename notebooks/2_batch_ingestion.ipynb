{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark Processing Script](#Create-PySpark-Processing-Script)\n",
    "1. [Run SageMaker Processing Job](#Run-SageMaker-Processing-Job)\n",
    "1. [Explore Aggregated Features](#Explore-Aggregated-Features)\n",
    "1. [Validate Feature Group for Records](#Validate-Feature-Group-for-Records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "- This notebook takes raw credit card transactions data (csv) generated by \n",
    "[notebook 0](./0_prepare_transactions_dataset.ipynb) and aggregates the raw features to create new features (ratios) via PySpark Job. These aggregated features alongside the raw original features will be leveraged in the training phase of a Credit Card Fraud Detection model in the next step (see notebook [notebook 3](./3_train_and_deploy_model.ipynb)).\n",
    "\n",
    "- As part of the Spark job, we also select the latest weekly aggregated features - `num_trans_last_1w` and `avg_amt_last_1w` grouped by `cc_num` (credit card number) and populate these features into the <b>SageMaker Online Feature Store</b> as a feature group. This feature group (`cc-agg-batch-fg`) was created in notebook [notebook 1](./1_setup.ipynb).\n",
    "\n",
    "- [Hopsworks Processing](https://hopsworks.ai) lets customers run analytics jobs for data engineering and model evaluation on Hopsworks easily and at scale. It provides a fully managed Spark environment for data processing or feature engineering workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to Hopsworks Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import desc, dense_rank\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from  argparse import Namespace, ArgumentParser\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "TOTAL_UNIQUE_USERS = 100 #10000\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "    \n",
    "schema = StructType([StructField('tid', StringType(), True),\n",
    "                         StructField('datetime', TimestampType(), True),\n",
    "                         StructField('cc_num', LongType(), True),\n",
    "                         StructField('amount', DoubleType(), True),\n",
    "                         StructField('fraud_label', StringType(), True)])\n",
    "\n",
    "# aggregated_features\n",
    "#transactions_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"hdfs:///Projects/realtime/Resources/transactions.csv\")\n",
    "\n",
    "transactions_df = spark.read.csv(path=\"hdfs:///Projects/realtime/Resources/transactions.csv\", inferSchema=True, header=True, sep=\",\") #schema=schema, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- tid: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-------------------+-----------+--------------------+\n",
      "|amount|          cc_num|           datetime|fraud_label|                 tid|\n",
      "+------+----------------+-------------------+-----------+--------------------+\n",
      "|  19.9|4653672048903767|2020-03-15 15:06:09|          0|df9058c1c293e6bd3...|\n",
      "|  36.6|4170245277417751|2020-03-15 15:10:11|          0|b9902ddccead2a1ca...|\n",
      "| 26.28|4978667132535671|2020-03-15 15:20:11|          0|2985420e04a29c7e1...|\n",
      "|822.53|4332149413304557|2020-03-15 15:20:31|          0|e41a745ed61f9d875...|\n",
      "| 652.2|4676706014866559|2020-03-15 15:26:45|          0|34108c0ff7bb8af9b...|\n",
      "| 48.34|4978667132535671|2020-03-15 15:36:04|          0|3997b0cb3d7d39f02...|\n",
      "| 18.55|4444037300542691|2020-03-15 15:41:04|          0|ac80ce12bcc81aa6e...|\n",
      "| 67.22|4789490563144262|2020-03-15 15:45:31|          0|dd3a6f7ec38653709...|\n",
      "|971.94|4829328237114208|2020-03-15 16:05:12|          0|553359d0e3e453bce...|\n",
      "| 89.96|4829328237114208|2020-03-15 16:06:34|          0|f8317ecbb278d27df...|\n",
      "| 84.54|4645884898081724|2020-03-15 16:22:58|          0|4522f2106ec6f2c33...|\n",
      "|256.21|4148950609641791|2020-03-15 16:24:13|          0|4e776b65a875d93f3...|\n",
      "| 34.44|4978667132535671|2020-03-15 16:25:02|          0|5ac88881ac009d9ac...|\n",
      "| 99.04|4301684712758167|2020-03-15 16:28:03|          0|3d842a0a13798e5f8...|\n",
      "|  4.42|4564139086560436|2020-03-15 16:32:47|          0|91967b945c4a43d9c...|\n",
      "|  48.7|4526611032294580|2020-03-15 16:32:56|          0|71385e99dceaf1aff...|\n",
      "|  42.8|4446288810992896|2020-03-15 16:44:07|          0|3a748dd997a473163...|\n",
      "|  1.64|4223253728365626|2020-03-15 16:49:15|          0|adbd1a216df5e6d05...|\n",
      "| 64.31|4532606684286514|2020-03-15 16:49:37|          0|d958d53d2ef32a904...|\n",
      "| 36.37|4522929944170142|2020-03-15 16:51:15|          0|959da5e697859c158...|\n",
      "+------+----------------+-------------------+-----------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT *, \\\n",
    "           avg_amt_last_10m/avg_amt_last_1w AS amt_ratio1, \\\n",
    "           amount/avg_amt_last_1w AS amt_ratio2, \\\n",
    "           num_trans_last_10m/num_trans_last_1w AS count_ratio \\\n",
    "    FROM \\\n",
    "        ( \\\n",
    "        SELECT *, \\\n",
    "               COUNT(*) OVER w1 as num_trans_last_10m, \\\n",
    "               AVG(amount) OVER w1 as avg_amt_last_10m, \\\n",
    "               COUNT(*) OVER w2 as num_trans_last_1w, \\\n",
    "               AVG(amount) OVER w2 as avg_amt_last_1w \\\n",
    "        FROM transactions_df \\\n",
    "        WINDOW \\\n",
    "               w1 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 10 MINUTE PRECEDING), \\\n",
    "               w2 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 1 WEEK PRECEDING) \\\n",
    "        ) \n",
    "    \"\"\"\n",
    "transactions_df.registerTempTable('transactions_df')\n",
    "aggregated_features = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\n# TODO: will do this for TD transformation\\n\\nsliced_df = grouped_df.select('cc_num', 'num_trans_last_1w', 'avg_amt_last_1w')\\n\\n# process rows\\nrecords = []\\nfor row in sliced_df.rdd.collect():\\n        record = []\\n        cc_num, num_trans_last_1w, avg_amt_last_1w = row\\n        if cc_num:\\n            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\\n            record.append({'ValueAsString': str(num_trans_last_1w), 'FeatureName': 'num_trans_last_1w'})\\n            record.append({'ValueAsString': str(round(avg_amt_last_1w, 2)), 'FeatureName': 'avg_amt_last_1w'})\\n            records.append(record)\\n\\n# write_to_feature_store\\nsuccess, fail = 0, 0\\nfor record in records:\\n        event_time_feature = {\\n                'FeatureName': 'trans_time',\\n                'ValueAsString': str(int(round(time.time())))\\n            }\\n        record.append(event_time_feature)\\n        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\\n        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\\n            success += 1\\n        else:\\n            fail += 1\\nassert success == TOTAL_UNIQUE_USERS\\nassert fail == 0\\n\""
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "aggregated_features.coalesce(1) \\\n",
    "                   .write.format('com.databricks.spark.csv') \\\n",
    "                   .option('header', True) \\\n",
    "                   .mode('overwrite') \\\n",
    "                   .option('sep', ',') \\\n",
    "                   .save('s3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix))\n",
    "\"\"\"\n",
    "# group_by_card_number\n",
    "window = Window.partitionBy('cc_num').orderBy(desc('datetime'))\n",
    "sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "grouped_df = sorted_df.filter(sorted_df.rank == 1).drop(sorted_df.rank)\n",
    "\n",
    "\"\"\"\n",
    "# TODO: will do this for TD transformation\n",
    "\n",
    "sliced_df = grouped_df.select('cc_num', 'num_trans_last_1w', 'avg_amt_last_1w')\n",
    "\n",
    "# process rows\n",
    "records = []\n",
    "for row in sliced_df.rdd.collect():\n",
    "        record = []\n",
    "        cc_num, num_trans_last_1w, avg_amt_last_1w = row\n",
    "        if cc_num:\n",
    "            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_1w), 'FeatureName': 'num_trans_last_1w'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_1w, 2)), 'FeatureName': 'avg_amt_last_1w'})\n",
    "            records.append(record)\n",
    "\n",
    "# write_to_feature_store\n",
    "success, fail = 0, 0\n",
    "for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'trans_time',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "assert success == TOTAL_UNIQUE_USERS\n",
    "assert fail == 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- datetime: timestamp (nullable = true)\n",
      " |-- fraud_label: integer (nullable = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- num_trans_last_10m: long (nullable = false)\n",
      " |-- avg_amt_last_10m: double (nullable = true)\n",
      " |-- num_trans_last_1w: long (nullable = false)\n",
      " |-- avg_amt_last_1w: double (nullable = true)\n",
      " |-- amt_ratio1: double (nullable = true)\n",
      " |-- amt_ratio2: double (nullable = true)\n",
      " |-- count_ratio: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "grouped_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           datetime|\n",
      "+-------------------+\n",
      "|2020-05-31 18:31:38|\n",
      "|2020-05-31 19:57:01|\n",
      "|2020-05-31 20:45:54|\n",
      "|2020-05-31 23:43:01|\n",
      "|2020-05-31 23:37:09|\n",
      "+-------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "grouped_df.select(\"datetime\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "grouped_df = grouped_df.withColumn(\"date\", f.from_unixtime(f.unix_timestamp(f.col(\"datetime\")), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "|2020-05-31|\n",
      "+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "grouped_df.select(\"date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (davit): investigate this\n",
    "#grouped_df = grouped_df.repartition(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "\n",
    "connection = hsfs.connection()\n",
    "# get a reference to the feature store, you can access also shared feature stores by providing the feature store name\n",
    "fs = connection.get_feature_store();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fe08cc7da50>"
     ]
    }
   ],
   "source": [
    "transactions_fg = fs.create_feature_group(\n",
    "    name = \"transactions_fg\", \n",
    "    description = \"Transactions Feature Group\",\n",
    "    version=1,\n",
    "    primary_key = [\"tid\"], \n",
    "    partition_key = [\"date\"], \n",
    "    time_travel_format = None\n",
    ")\n",
    "transactions_fg.save(grouped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Explore Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Validate Feature Group for Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
